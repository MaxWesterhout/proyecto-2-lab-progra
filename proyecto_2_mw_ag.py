# -*- coding: utf-8 -*-
"""Proyecto_2_MW_AG

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Iby14Vk-MG0Lu9pnIocvkpnplTGKqGS

![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)

# Proyecto: Riesgo en el Banco Giturra

**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**

### Cuerpo Docente:

- Profesor: Gabriel Iturra, Ignacio Meza De La Jara
- Auxiliar: Sebastián Tinoco
- Ayudante: Arturo Lazcano, Angelo Muñoz

_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._

---

## Reglas

- Fecha de entrega: 19/12/2023
- **Grupos de 2 personas.**
- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.
- Estrictamente prohibida la copia.
- Pueden usar cualquier material del curso que estimen conveniente.

---
"""

https://github.com/MaxWesterhout/proyecto-2-lab-progra.git

"""# Presentación del Problema

![](https://www.diarioeldia.cl/u/fotografias/fotosnoticias/2019/11/8/67218.jpg)

**Giturra**, un banquero astuto y ambicioso, estableció su propio banco con el objetivo de obtener enormes ganancias. Sin embargo, su reputación se vio empañada debido a las tasas de interés usureras que imponía a sus clientes. A medida que su banco crecía, Giturra enfrentaba una creciente cantidad de préstamos impagados, lo que amenazaba su negocio y su prestigio.

Para abordar este desafío, Giturra reconoció la necesidad de reducir los riesgos de préstamo y mejorar la calidad de los préstamos otorgados. Decidió aprovechar la ciencia de datos y el análisis de riesgo crediticio. Contrató a un equipo de expertos para desarrollar un modelo predictivo de riesgo crediticio.

Cabe señalar que lo modelos solicitados por el banquero deben ser interpretables. Ya que estos le permitira al equipo comprender y explicar cómo se toman las decisiones crediticias. Utilizando visualizaciones claras y explicaciones detalladas, pudieron identificar las características más relevantes, le permitirá analizar la distribución de la importancia de las variables y evaluar si los modelos son coherentes con el negocio.

Para esto Giturra les solicita crear un modelo de riesgo disponibilizandoles una amplia gama de variables de sus usuarios: como historiales de crédito, ingresos y otros factores financieros relevantes, para evaluar la probabilidad de incumplimiento de pago de los clientes. Con esta información, Giturra podra tomar decisiones más informadas en cuanto a los préstamos, ofreciendo condiciones más favorables a aquellos con menor riesgo de impago.

## Instalación de Librerías y Carga de Datos.

Para el desarrollo de su proyecto, utilice el conjunto de datos `dataset.pq` para entrenar un modelo de su elección. Se le recomienda levantar un ambiente de `conda` para instalar las librerías y así evitar cualquier problema con las versiones.

---

## Secciones Requeridas en el Informe

La siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto.
Es importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.

### 1. Introducción [0.5 puntos]

_Esta sección es literalmente una muy breve introducción con todo lo necesario para entender que hicieron en su proyecto._

- Describir brevemente el problema planteado (¿Qué se intenta predecir?)
- Describir brevemente los datos de entrada que les provee el problema.
- Describir las métricas que utilizarán para evaluar los modelos generados. Eligan **una métrica** adecuada para el desarrollo del proyecto **según la tarea que deben resolver y la institución a la cuál será su contraparte** y luego justifiquen su elección. Considerando que los datos presentan desbalanceo y que el uso de la métrica 'accuracy' sería incorrecto, enfoquen su elección en una de las métricas precision, recall o f1-score y en la clase que será evaluada.
- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).
- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos.

### 2. Carga de datos Análisis Exploratorio de Datos [Sin puntaje]

_La idea de esta sección es que cargen y exploren el dataset para así obtener una idea de como son los datos y como se relacionan con el problema._

Cargue los datos y realice un análisis exploratorio de datos para investigar patrones, tendencias y relaciones en un conjunto de datos. Se adjuntan diversos scripts para abodar rápidamente este punto. La descripción de las columnas las pueden encontrar en el siguiente [enlace](https://www.kaggle.com/datasets/parisrohan/credit-score-classification).

**NO deben escribir nada**, solo ejecutar el código y encontrar los patrones con los cuales se basaran para generar el modelo.

### 3. Preparación de Datos [0.5 puntos]

_Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo._

#### 3.1 Preprocesamiento con `ColumnTransformer`

- Convierta las columnas mal leidas a sus tipos correspondientes (float, str, etc...)
- Genere un `ColumnTransformer` que:
  - Preprocese datos categóricos y ordinales.
  - Escale/estandarice datos numéricos.
  - Uitlice `.set_output(transform="pandas")` sobre su `ColumnTransformer` para setear el formato de salida a de las transformaciones a pandas.

- Luego, pruebe las transformaciones utilizando `fit_transform`.

- Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos.

#### 3.2 Holdout

Ejecute `train_test_split` para generar un conjunto de entrenamiento, validacióny de prueba.

#### 3.3 Datos nulos.

Como habrá visto, existe la posibilidad de que algunos datos sean nulos. En esta sección se le solicita justificar, previo a comenzar el modelado, decidir si conservar e imputar los datos nulos o eliminar las filas.

Note que la decisión que tomen aquí puede afectar fuertemente el rendimiento de los modelos.
Y como siempre, más adelante tienen el espacio para experimentar con ambas opciones.

#### 3.4 Feature Engineering [Bonus - 0.5 puntos]

En esta sección, se espera que apliquen su conocimiento y creatividad para identificar y construir características que brinden una mejor orientación a su modelo para identificar los casos deseados. Para motivar la construcción de nuevas características, se recomienda explorar las siguientes posibilidades:

- Generar ratios que relacionen variables categóricas con numéricas. Estos ratios permiten capturar relaciones proporcionales o comparativas entre diferentes categorías y valores numéricos.
- Combinación de rankings entre variables numéricas y categóricas.
- Discretización de variables numéricas a categóricas.
- Etc...

**Importantes**: Al explorar estas posibilidades no se limiten solo a estas propuestas, pueden aplicar otras técnicas de feature engineering pertinentes para mejorar la capacidad de su modelo para comprender y aprovechar los patrones presentes en los datos.

### 4. Baseline [1.5 puntos]

_En esta sección deben crear los modelos más básicos posibles que resuelvan el problema dado. La idea de estos modelos son usarlos como comparación para que en el siguiente paso lo puedan mejorar._

Implemente, entrene y evalúe varias `Pipeline` enfocadas en resolver el problema de clasificación en donde la diferencia entre estas sea el modelo utilizado.


Para esto, cada Pipeline debe:

- Tener el `ColumnTransformer` implementado en la sección anterior como primer paso.
- Implementar un imputador en caso de haber decidido conservar los datos nulos.
- Implementar un clasificador en la salida (ver siguiente lista).
  
Y además,
- Ser evaluado de forma general imprimiendo un `classification_report`.
- Calcular y guardar la métrica seleccionada en el punto 1.2 en un arreglo de métricas (guardar nombre y valor de la métrica).

Lo anterior debe ser implementado utilizando los siguientes modelos:

- `Dummy` con estrategia estratificada.
- `LogisticRegression`.
- `KNeighborsClassifier`.
- `DecisionTreeClassifier`
- `SVC`
- `RandomForestClassifier`
- `LightGBMClassifier` (del paquete `lightgbm`)
- `XGBClassifier` (del paquete `xgboost`).


Luego, transformando el diccionario de las métricas a un pandas `DataFrame`, ordene según los valores de su métrica de mayor a menor y responda.
- ¿Hay algún clasificador entrenado mejor que el azar (`Dummy`)?
- ¿Cuál es el mejor clasificador entrenado?
- ¿Por qué el mejor clasificador es mejor que los otros?
- Respecto al tiempo de entrenamiento, con cual cree que sería mejor experimentar (piense en el tiempo que le tomaría pasar el modelo por una grilla de optimización de hiperparámetros).

**Nota**: Puede utilizar un for más una lista con las clases de los modelos mencionados para simplificar el proceso anterior.


### 5. Optimización del Modelo [1.5 puntos]

_En esta sección deben mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando._

- Instanciar dos nuevas `Pipeline`, similares a la anterior, pero ahora enfocada en buscar el mejor modelo. Para esto, la pipelines debe utilizar el primer y segundo mejor modelo encontrado en el paso anterior.
- Usar **`Optuna`** para tunear hiperparámetros
- **Importante**: Recuerden setear la búsqueda para optimizar la métrica seleccionada en los puntos anteriores.

Algunas ideas para mejorar el rendimiento de sus modelos:

- Agregar técnicas de seleccion de atributos/características. El parámetro de cuántas características se seleccionan debe ser parametrizable y configurado por el optimizador de hiperparámetros.
- Variar el imputador de datos en caso de usarlo.

#### Bonus

1. **Visualización con Optuna** [0.2 extras]: Explore la documentación de visualización de Optuna en el siguiente [link](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) y realice un análisis sobre el proceso de optimización de hiperparámetros realizado.
2. **Imabalanced learn** [0.3 extras]: Al ser el problema desbalanceado, pueden probar técnicas para balancear automáticamente el dataset previo a ejecutar el modelo. Para esto, puede probar con los mecanismos implementados en la librería [Imbalanced learn](https://imbalanced-learn.org/).
3. **Probar pycaret (AutoML)** [0.3 extras].

Algunas notas interesantes sobre este proceso:

- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.
- **Hacer grillas computables**: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar.
- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.

**Al final de este proceso, seleccione el mejor modelo encontrado, prediga el conjunto de prueba y reporte sus resultados.**

### 6. Interpretabilidad [1.0 puntos]

_En esta sección, se espera que los estudiantes demuestren su capacidad para explicar cómo sus modelos toman decisiones utilizando los datos. Dentro del análisis de interpretabilidad propuesto para el modelo, deberán ser capaces de:_

- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.
- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?
- Analizar 10 observaciones aleatorias utilizando un método específico para verificar la coherencia de las interacciones entre las características.
- Explorar cómo se relacionan las variables utilizando algún descriptivo de interpretabilidad.
- ¿Existen variables irrelevantes en el problema modelado?, ¿Cuales son?.

Es fundamental que los estudiantes sean capaces de determinar si su modelo toma decisiones coherentes y evaluar el impacto que podría tener la aplicación de un modelo con esas variables en una población. ¿Es posible que el modelo sea perjudicial o que las estimaciones se basen en decisiones sesgadas?

En resumen, esta sección busca que los estudiantes apliquen un enfoque crítico para evaluar la interpretabilidad de su modelo, identificar posibles sesgos y analizar las implicaciones de sus decisiones en la población objetivo.

### 7. Concluir [1.0 puntos]

_Aquí deben escribir una breve conclusión del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:_

- ¿Pudieron resolver exitosamente el problema?
- ¿Son aceptables los resultados obtenidos?
- ¿En que medida el EDA ayudó a comprender los datos en miras de generar un modelo predictivo?

Respecto a la clasificación:

- ¿Como fue el rendimiento del baseline para la clasificación?
- ¿Pudieron optimizar el baseline para la clasificación?
- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?

Finalmente:

- ¿Estuvieron conformes con sus resultados?
- ¿Creen que hayan mejores formas de modelar el problema?
- ¿En general, qué aprendieron del proyecto? ¿Qué no aprendieron y les gustaría haber aprendido?

**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.

### Otras Instrucciones

Recordar el uso de buenas prácticas de MLOPS como replicabilidad (fijar semillas aleatorias) o el uso del registro de experimentos (con MLFlow). Si bien son opcionales, es altamente recomendado su uso.

### 8. Bonus: Implementación de Kedro y FastAPI [1.5 puntos]

**OPCIONAL**

En esta sección se les solicita utilizar las últimas tecnologías vistas en el curso para la productivización del proyecto de ciencia de datos, centrándose en la organización y gestión de los flujos de trabajo a través de componentes y pipelines, más el servicio del modelo a través del desarrollo de una API.

Para esto:

1. Genere un proyecto de `Kedro` en donde separe por responsabilidades los nodos/componentes de su proyecto de ciencia de datos en módulos separados. [1.0 puntos]
2. Genere un servidor basado en `FastAPI` el cuál a través de un método post, reciba un batch de datos y genere predicciones para cada uno de ellos. [0.5 puntos]

Las implementaciones son libres. Es decir, usted decide qué componentes implementar, como usar el catálogo de datos y la parametrización del flujo. Sin embargo, evaluaremos buen uso de los framework, modularización y separación de responsabilidades.

<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target="_blank">
<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>
Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>

##Introducción [0.5 puntos]

En el contexto bancario, la evaluación del riesgo crediticio es esencial para la estabilidad financiera, por ello, se busca predecir la probabilidad de incumplimiento de pago por parte de los clientes de Giturra, un banco cuya reputación se ha visto afectada debido a tasas de interés desproporcionadas y un creciente número de préstamos impagados. Con el propósito de mitigar estos riesgos, se dispone de una amplia gama de variables financieras de los usuarios, incluyendo historiales crediticios, ingresos y otros factores relevantes.

Los datos proporcionados incluyen una serie de atributos como edad, ocupación, ingresos anuales, número de cuentas bancarias, historial de pagos, entre otros. Estos datos servirán como base para la creación de un modelo predictivo de riesgo crediticio.

En la evaluación de los modelos generados, se ha optado por utilizar la métrica de precisión (precision) debido al desbalanceo presente en los datos. La precisión se enfoca en la proporción de predicciones positivas (clientes catalogados como de alto riesgo) que son verdaderamente correctas entre todas las predicciones positivas realizadas. Dado el interés en identificar con alta certeza aquellos clientes que podrían representar un riesgo crediticio, la precisión es crucial en este contexto.

En cuanto al modelo empleado, se exploraron dos enfoques principales: Random Forest Classifier y LGBMBoost. Tras un proceso de optimización, se determinó que el modelo de Random Forest Classifier ofrecía mejores resultados en la predicción del riesgo crediticio, superando al otro modelo en términos de rendimiento.

El modelo desarrollado ha demostrado ser efectivo al abordar el desafío de predicción de incumplimiento de pago. Los resultados obtenidos con el Random Forest Classifier han sido satisfactorios, permitiendo identificar con precisión aquellos clientes con mayor riesgo de impago., ofreciendo a Giturra una herramienta valiosa para tomar decisiones más fundamentadas en la gestión de préstamos y así proteger la estabilidad financiera del banco.

##Carga de datos Análisis Exploratorio de Datos
"""

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.impute import SimpleImputer
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_parquet('dataset.pq')
df.sample(5)

"""analizamos el tipo de las variables"""

df.info()

"""vemos los nulos"""

df.isna().sum()

"""analizamos las estadisticas"""

df.describe()

"""analizamos los valores unicos por variables para ver si existen categoricas"""

df.nunique()

"""Podemos ver que ocupation, payment_of_min_amount, payment_behaviour son variables categoricas

##Preparación de los Datos [0.5 puntos]

- Convierta las columnas mal leidas a sus tipos correspondientes (float, str, etc...)
- Genere un `ColumnTransformer` que:
  - Preprocese datos categóricos y ordinales.
  - Escale/estandarice datos numéricos.
  - Uitlice `.set_output(transform="pandas")` sobre su `ColumnTransformer` para setear el formato de salida a de las transformaciones a pandas.

- Luego, pruebe las transformaciones utilizando `fit_transform`.

- Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos.

###Preprocesamiento con ColumnTransformer
"""

categorical = ['occupation','payment_of_min_amount','payment_behaviour','customer_id']
numerical = ['age', 'annual_income', 'num_bank_accounts', 'num_credit_card', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'changed_credit_limit', 'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance','monthly_inhand_salary','credit_history_age','num_of_delayed_payment']

for col in numerical:
    df[col] = pd.to_numeric(df[col], errors='coerce')
for col in categorical:
    df[col] = df[col].astype('category')

"""customer id	Identificación del cliente

age	edad

occupation	ocupación

annual income	ingresos anuales

monthly inhand salary	salario interno mensual

num bank accounts	numero de cuentas bancarias

num credit card	número de tarjeta de crédito

interest rate	tasa de interés

num of loan	número de préstamo

delay from due date	retraso desde la fecha de vencimiento

num of delayed payment	número de pagos retrasados

changed credit limit	límite de crédito cambiado

num credit inquiries	número de consultas de crédito

outstanding debt	deuda pendiente

credit utilization ratio	índice de utilización del crédito

credit history age	edad del historial crediticio

payment of min amount	pago del monto mínimo

total emi per month	total de emi por mes

amount invested monthly	cantidad invertida mensualmente

payment behaviour	comportamiento de pago

monthly balance	saldo mensual

credit score	puntuación de crédito
"""

transformer = ColumnTransformer([
    ('Categoricas', OneHotEncoder(sparse=False, handle_unknown="ignore",drop='if_binary'), categorical),
    ('Numericas', RobustScaler(), numerical),
],remainder='passthrough')

transformer.set_output(transform="pandas")

df_column_t = transformer.fit_transform(df)
display(df_column_t)
del df_column_t

"""Notar que la variable costumer_id explota el one_hot_encoder dado que dicha variable contiene una cantidad enorme de datos asociados a ids (valores unicos), por ello debemos quitar dicha columna para hacer efectivo este analisis.

###Holdout

Ejecute train_test_split para generar un conjunto de entrenamiento, validacióny de prueba.
"""

# conjunto de entrenamiento (70%), validación (20%) y prueba (10%)
from sklearn.model_selection import train_test_split

def hold_out(df):
  val_size = 0.2
  test_size = 0.1

  x = df.drop(columns=['credit_score'])
  y = df['credit_score']

  var_1 = val_size + test_size
  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = var_1, random_state=42)

  var_2 = (test_size*len(x)/100)*100/len(x_val)
  x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.33, random_state=42)

  return x_train, y_train, x_test, y_test, x_val, y_val

"""###Datos Nulos

Como habrá visto, existe la posibilidad de que algunos datos sean nulos. En esta sección se le solicita justificar, previo a comenzar el modelado, decidir si conservar e imputar los datos nulos o eliminar las filas.

Note que la decisión que tomen aquí puede afectar fuertemente el rendimiento de los modelos. Y como siempre, más adelante tienen el espacio para experimentar con ambas opciones.

filtramos debido a que estas variables no deberian tener numeros negativos

edad

num_bank_accounts
"""

df.isna().sum().sort_values(ascending=False)*100 / len(df)

"""al estar en % podemos considerar que las que esten sobre un 6% no se considerarán como columnas validas, las demás se imputaran con el promedio de su columna"""

#Aplicamos preprocesamiento
categorical = ['occupation','payment_of_min_amount','payment_behaviour']
numerical = ['age', 'annual_income', 'num_bank_accounts', 'num_credit_card', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'changed_credit_limit', 'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance','monthly_inhand_salary','credit_history_age','num_of_delayed_payment']

df_pre=df.copy()

df_pre = df_pre.drop(columns=['customer_id'])

for col in numerical:
  df[col] = pd.to_numeric(df[col], errors='coerce')
for col in categorical:
  df[col] = df[col].astype('category')

df_pre = df_pre[df_pre['age'] >= 0]
df_pre = df_pre[df_pre['num_bank_accounts'] >= 0]
nulos = ['amount_invested_monthly', 'monthly_balance', 'num_credit_inquiries', 'changed_credit_limit']

#Separamos la data con el hold_out

x_train, y_train, x_test, y_test, x_val, y_val = hold_out(df_pre)
x_train

#Redifinimos transformer sin costumer id
categorical = ['occupation','payment_of_min_amount','payment_behaviour']
numerical = ['age', 'annual_income', 'num_bank_accounts', 'num_credit_card', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'changed_credit_limit', 'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance','monthly_inhand_salary','credit_history_age','num_of_delayed_payment']

transformer = ColumnTransformer([
    ('Categoricas', OneHotEncoder(sparse=False, handle_unknown="ignore",drop='if_binary'), categorical),
    ('Numericas', RobustScaler(), numerical),
],
    remainder='passthrough')
transformer.set_output(transform="pandas")

#Creaos un imputer
imputer = SimpleImputer(strategy='mean')

"""Se considera la media dado que por laboratorios anteriores consideramos que es el mejor metodo, ademas la imputacion ocurre despues del robust scaler por ello la media no deberia afectar en gran medida.

### Feature Engineering [Bonus - 0.5 puntos]

En esta sección, se espera que apliquen su conocimiento y creatividad para identificar y construir características que brinden una mejor orientación a su modelo para identificar los casos deseados. Para motivar la construcción de nuevas características, se recomienda explorar las siguientes posibilidades:

Generar ratios que relacionen variables categóricas con numéricas. Estos ratios permiten capturar relaciones proporcionales o comparativas entre diferentes categorías y valores numéricos.
Combinación de rankings entre variables numéricas y categóricas.
Discretización de variables numéricas a categóricas.
Etc...
Importantes: Al explorar estas posibilidades no se limiten solo a estas propuestas, pueden aplicar otras técnicas de feature engineering pertinentes para mejorar la capacidad de su modelo para comprender y aprovechar los patrones presentes en los datos.

##Baseline [1.5 puntos]

En esta sección deben crear los modelos más básicos posibles que resuelvan el problema dado. La idea de estos modelos son usarlos como comparación para que en el siguiente paso lo puedan mejorar.

Implemente, entrene y evalúe varias Pipeline enfocadas en resolver el problema de clasificación en donde la diferencia entre estas sea el modelo utilizado.

Para esto, cada Pipeline debe:

Tener el ColumnTransformer implementado en la sección anterior como primer paso.
Implementar un imputador en caso de haber decidido conservar los datos nulos.
Implementar un clasificador en la salida (ver siguiente lista).
Y además,

Ser evaluado de forma general imprimiendo un classification_report.
Calcular y guardar la métrica seleccionada en el punto 1.2 en un arreglo de métricas (guardar nombre y valor de la métrica).
Lo anterior debe ser implementado utilizando los siguientes modelos:

Dummy con estrategia estratificada.
LogisticRegression.
KNeighborsClassifier.
DecisionTreeClassifier
SVC
RandomForestClassifier
LightGBMClassifier (del paquete lightgbm)
XGBClassifier (del paquete xgboost).
Luego, transformando el diccionario de las métricas a un pandas DataFrame, ordene según los valores de su métrica de mayor a menor y responda.

¿Hay algún clasificador entrenado mejor que el azar (Dummy)?
¿Cuál es el mejor clasificador entrenado?
¿Por qué el mejor clasificador es mejor que los otros?
Respecto al tiempo de entrenamiento, con cual cree que sería mejor experimentar (piense en el tiempo que le tomaría pasar el modelo por una grilla de optimización de hiperparámetros).
Nota: Puede utilizar un for más una lista con las clases de los modelos mencionados para simplificar el proceso anterior.
"""

def create_pipeline(ColumTransformer,model):
    return Pipeline(steps=[
        ("transform", ColumTransformer),
        ("imputer", imputer),
        ("model", model)])

seed = 42
results_df = pd.DataFrame(columns=['Model', 'Precision_Val', 'Precision_Test','Accuracy_val','Accuracy_Test','tiempo [seg]'])
results_df

#Librerias para calcular tiempo
import time

"""###Dummy"""

#dummy = create_pipeline(Preprocesamiento, transformer, holdout, DummyClassifier(strategy='stratified', random_state=seed))
dummy = create_pipeline(transformer, DummyClassifier(strategy='stratified', random_state=seed))

dummy.fit(x_train,y_train)

t1 = time.time()

y_pred_val = dummy.predict(x_val)
y_pred_test = dummy.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm= confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])



conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='d', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'Dummy',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""### LogisticRegression"""

t1 = time.time()

logistic_regression = create_pipeline(transformer, LogisticRegression(random_state=seed))
logistic_regression.fit(x_train, y_train)

y_pred_val = logistic_regression.predict(x_val)
y_pred_test = logistic_regression.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'logistic regression',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""###KNeighborsClassifier"""

t1 = time.time()

KNeighbors_Classifier = create_pipeline(transformer, KNeighborsClassifier())
KNeighbors_Classifier.fit(x_train, y_train)

y_pred_val = KNeighbors_Classifier.predict(x_val)
y_pred_test = KNeighbors_Classifier.predict(x_test)

# metricas
print("\n Validacion:")
conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'KNeighbors_Classifier',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""### DecisionTreeClassifier"""

t1 = time.time()

DecisionTree_Classifier = create_pipeline(transformer, DecisionTreeClassifier(random_state=seed))
DecisionTree_Classifier.fit(x_train, y_train)

y_pred_val = DecisionTree_Classifier.predict(x_val)
y_pred_test = DecisionTree_Classifier.predict(x_test)

# metricas
print("\n Validacion:")
conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'DecisionTree_Classifier',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

""" ### SVC"""

t1 = time.time()

svc = create_pipeline( transformer, SVC(random_state=seed))
svc.fit(x_train, y_train)

y_pred_val = svc.predict(x_val)
y_pred_test = svc.predict(x_test)

# metricas
print("\n Validacion:")
conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'svc',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""### RandomForestClassifier"""

t1 = time.time()

RandomForest_Classifier = create_pipeline(transformer, RandomForestClassifier(random_state=seed))
RandomForest_Classifier.fit(x_train, y_train)

y_pred_val = RandomForest_Classifier.predict(x_val)
y_pred_test = RandomForest_Classifier.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'RandomForest_Classifier',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""### LightGBMClassifier"""

t1 = time.time()

LGBM_Classifier = create_pipeline(transformer, LGBMClassifier(random_state=seed))
LGBM_Classifier.fit(x_train, y_train)

y_pred_val = LGBM_Classifier.predict(x_val)
y_pred_test = LGBM_Classifier.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'LGBM_Classifier',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

"""### XGBClassifier"""

t1 = time.time()

XGB_Classifier = create_pipeline(transformer, XGBClassifier(random_state=seed))
XGB_Classifier.fit(x_train, y_train)

y_pred_val = XGB_Classifier.predict(x_val)
y_pred_test = XGB_Classifier.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

t2 = time.time()
tiempo_total_segundos_1 = t2 - t1

results_df = results_df.append({
    'Model': 'XGB_Classifier',
    'Precision_Val': precision_val,
    'Precision_Test': precision_test,
    'Accuracy_val' : accuracy_val,
    'Accuracy_Test' : accuracy_test,
    'tiempo [seg]': tiempo_total_segundos_1
}, ignore_index=True)

results_df

results_df.sort_values(by = ['Precision_Test'],ascending=False)

results_df.sort_values(by = ['Accuracy_Test'],ascending=False)

"""Con estos resultados notamos que los mejores clasificadores son LGBM, XGB y RandomForest. Si analizamos la metrica de precision, accuracy y tiempo, notamos que el mejor corresponde a LGBM, luego RandomForest y por ultimo XGB, siendo estos ultimos dos clasificadores que toman mucho mas tiempo.

Tras  analizar los resultados obtenidos de diversos modelos de clasificación, se ha tomado la decisión de avanzar con el modelo RandomForest_Classifier. Los resultados revelaron que este modelo, junto con el LGBM_Classifier, supera notablemente el rendimiento del clasificador Dummy, destacándose como los mejores modelos entrenados en términos de precisión y exactitud en la predicción del riesgo crediticio.

Dentro de estos dos, se ha optado por el RandomForest_Classifier debido a su capacidad inherente para abordar relaciones no lineales y complejas entre las variables. Este modelo se basa en la técnica de ensamblaje, combinando múltiples árboles de decisión para formar un clasificador robusto. Cada árbol en el bosque se entrena de forma independiente, utilizando diferentes subconjuntos de datos y características, y luego se combinan sus resultados para obtener una predicción final. Esto permite capturar patrones no lineales y relaciones complejas presentes en los datos, aportando así una mayor flexibilidad al modelo.

Además, el RandomForest_Classifier tiene la capacidad de reducir el sobreajuste que podría surgir en modelos más complejos, como por ejemplo el DecisionTree_Classifier. Al construir múltiples árboles y tomar decisiones basadas en la combinación de sus resultados, este modelo tiende a ser más resistente al ruido y a las características irrelevantes en los datos, lo que lo hace menos propenso a ajustarse excesivamente a los detalles específicos de entrenamiento y, en cambio, se enfoca en patrones más generales y relevantes para la predicción del riesgo crediticio

Si bien el tiempo de entrenamiento del RandomForest_Classifier puede ser más prolongado en comparación con algunos otros modelos, su capacidad para manejar eficazmente relaciones no lineales y complejas en los datos, así como su habilidad para mitigar el sobreajuste, lo posicionan como una elección sólida y confiable para abordar el desafío de la evaluación del riesgo crediticio en el contexto bancario.

## 5. Optimización del Modelo [1.5 puntos]

_En esta sección deben mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando._

- Instanciar dos nuevas `Pipeline`, similares a la anterior, pero ahora enfocada en buscar el mejor modelo. Para esto, la pipelines debe utilizar el primer y segundo mejor modelo encontrado en el paso anterior.
- Usar **`Optuna`** para tunear hiperparámetros
- **Importante**: Recuerden setear la búsqueda para optimizar la métrica seleccionada en los puntos anteriores.

Algunas ideas para mejorar el rendimiento de sus modelos:

- Agregar técnicas de seleccion de atributos/características. El parámetro de cuántas características se seleccionan debe ser parametrizable y configurado por el optimizador de hiperparámetros.
- Variar el imputador de datos en caso de usarlo.

#### Bonus

1. **Visualización con Optuna** [0.2 extras]: Explore la documentación de visualización de Optuna en el siguiente [link](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) y realice un análisis sobre el proceso de optimización de hiperparámetros realizado.
2. **Imabalanced learn** [0.3 extras]: Al ser el problema desbalanceado, pueden probar técnicas para balancear automáticamente el dataset previo a ejecutar el modelo. Para esto, puede probar con los mecanismos implementados en la librería [Imbalanced learn](https://imbalanced-learn.org/).
3. **Probar pycaret (AutoML)** [0.3 extras].

Algunas notas interesantes sobre este proceso:

- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.
- **Hacer grillas computables**: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar.
- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.

**Al final de este proceso, seleccione el mejor modelo encontrado, prediga el conjunto de prueba y reporte sus resultados.**
"""

!pip install -qq xgboost optuna

import optuna

#Se fija la semilla
seed = 10
import numpy as np
np.random.seed(seed)

#Parametros del LGBM
parametros = {
'n_estimators':(50, 500),
'learning_rate':(0.001, 0.1),
'max_depth':(3, 10),
'subsample':(0.5, 1)}

from sklearn.metrics import mean_absolute_error
import warnings

categorical = ['occupation','payment_of_min_amount','payment_behaviour']
numerical = ['age', 'annual_income', 'num_bank_accounts', 'num_credit_card', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'changed_credit_limit', 'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance','monthly_inhand_salary','credit_history_age','num_of_delayed_payment']

warnings.filterwarnings('ignore')
# Desactivar warnings de LightGBM
warnings.filterwarnings("ignore", category=UserWarning, module="LightGBM")

model =  LGBMClassifier(set_seed = seed,verbose=-1)

#Sintaxis optuna
def objective(trial):
  # Definir el espacio de búsqueda para XGBRegressor
    LGBM_params = {
        'n_estimators': trial.suggest_int('n_estimators', *parametros['n_estimators']),
        'learning_rate': trial.suggest_loguniform('learning_rate', *parametros['learning_rate']),
        'max_depth': trial.suggest_int('max_depth', *parametros['max_depth']),
        'subsample': trial.suggest_loguniform('subsample', *parametros['subsample'])}

    transformer = ColumnTransformer([
        ('Categoricas', OneHotEncoder(sparse_output=False, handle_unknown="ignore",drop='if_binary'), categorical),
        ('Numericas', RobustScaler(), numerical),
    ],remainder='passthrough')

    transformer.set_output(transform="pandas")

    model.set_params(**LGBM_params)

    #Generacion pipeline
    pipeline = Pipeline(steps=[
    ("transform", transformer),
    ("imputer", imputer),
    ("model", model)])

    start_time = time.time()

    # Entrenar el modelo
    pipeline.fit(x_train,y_train)

    # Realizar predicciones en el conjunto de validación
    predictions_xgb = pipeline.predict(x_val)

    #calcular precision
    y_pred = pipeline.predict(x_val)
    precision = precision_score(y_val, predictions_xgb)

    print(f'Precision: {precision}')

    return precision

# Configurar optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)
sampler = optuna.samplers.TPESampler(seed=seed)
study_opt = optuna.create_study(direction='maximize', sampler=sampler)

# Ejecutar la optimización
study_opt.optimize(objective, timeout=300, show_progress_bar=True)

# Obtener los mejores hiperparámetros
best_params = study_opt.best_params
best_mae = study_opt.best_value
num_trials = len(study_opt.trials)

print(f'Número de trials: {num_trials}')
print(f'MAE con los mejores hiperparámetros: {best_mae}')
print(f'Mejores hiperparámetros: {best_params}')

#importamos joblib para guardar el modelo
import joblib

# Obtener el mejor modelo con los mejores hiperparámetros
LGBM_params = {
    'n_estimators': best_params['n_estimators'],
    'learning_rate': best_params['learning_rate'],
    'max_depth': best_params['max_depth'],
    'subsample': best_params['subsample']}

transformer = ColumnTransformer([
    ('Categoricas', OneHotEncoder(sparse_output=False, handle_unknown="ignore",drop='if_binary'), categorical),
    ('Numericas', RobustScaler(), numerical),
],remainder='passthrough')

transformer.set_output(transform="pandas")

model =  LGBMClassifier(verbose=-1)
model.set_params(**LGBM_params)

#Generacion pipeline
best_model = Pipeline(steps=[
("transform", transformer),
("imputer", imputer),
("model", model)])

# Entrenar el mejor modelo con todos los datos
best_model.fit(x_train, y_train)
val_predictions_xgb_opt = best_model.predict(x_val)
mae_xgb_opt = mean_absolute_error(y_val, val_predictions_xgb_opt)
print(f'MAE con LGBM Optimizado: {mae_xgb_opt}')
print(f'Número de trials: {num_trials}')
#Print parametros
print('')
print('Mejores hiperparámetros:')
for key, value in best_params.items():
    print(f'{key}: {value}')

# Guardar modelo
joblib.dump(best_model, 'best_model_optimized_primero.pkl')


y_pred_val = best_model.predict(x_val)
y_pred_test = best_model.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

#AHORA PARA RANDOM
#Se fija la semilla
seed = 10
import numpy as np
np.random.seed(seed)

#Parametros
parametros = {
'n_estimators':(50, 500),
'max_depth':(3, 23),
'max_leaf_nodes':(2, 200)}

categorical = ['occupation','payment_of_min_amount','payment_behaviour']
numerical = ['age', 'annual_income', 'num_bank_accounts', 'num_credit_card', 'interest_rate', 'num_of_loan', 'delay_from_due_date', 'changed_credit_limit', 'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance','monthly_inhand_salary','credit_history_age','num_of_delayed_payment']

model =  RandomForestClassifier(random_state=seed)

#Sintaxis optuna
def objective(trial):
  # Definir el espacio de búsqueda para XGBRegressor
    random_params = {
        'n_estimators': trial.suggest_int('n_estimators', *parametros['n_estimators']),
        'max_depth': trial.suggest_int('max_depth', *parametros['max_depth']),
        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', *parametros['max_leaf_nodes'])}

    transformer = ColumnTransformer([
        ('Categoricas', OneHotEncoder(sparse_output=False, handle_unknown="ignore",drop='if_binary'), categorical),
        ('Numericas', RobustScaler(), numerical),
    ],remainder='passthrough')

    transformer.set_output(transform="pandas")

    model.set_params(**random_params)

    #Generacion pipeline
    pipeline = Pipeline(steps=[
    ("transform", transformer),
    ("imputer", imputer),
    ("model", model)])

    start_time = time.time()

    # Entrenar el modelo
    pipeline.fit(x_train,y_train)

    # Realizar predicciones en el conjunto de validación
    predictions_xgb = pipeline.predict(x_val)

    #calcular precision
    y_pred = pipeline.predict(x_val)
    precision = precision_score(y_val, predictions_xgb)

    print(f'Precision: {precision}')

    return precision


# Configurar optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)
sampler = optuna.samplers.TPESampler(seed=seed)
study_opt = optuna.create_study(direction='maximize', sampler=sampler)

# Ejecutar la optimización
study_opt.optimize(objective, timeout=300, show_progress_bar=True)

# Obtener los mejores hiperparámetros
best_params = study_opt.best_params
best_mae = study_opt.best_value
num_trials = len(study_opt.trials)

print(f'Número de trials: {num_trials}')
print(f'MAE con los mejores hiperparámetros: {best_mae}')
print(f'Mejores hiperparámetros: {best_params}')

# Obtener el mejor modelo con los mejores hiperparámetros
best_random = {
    'n_estimators': best_params['n_estimators'],
    'max_depth': best_params['max_depth'],
    'max_leaf_nodes': best_params['max_leaf_nodes']}

transformer = ColumnTransformer([
    ('Categoricas', OneHotEncoder(sparse_output=False, handle_unknown="ignore",drop='if_binary'), categorical),
    ('Numericas', RobustScaler(), numerical),
],remainder='passthrough')

transformer.set_output(transform="pandas")

model = RandomForestClassifier(random_state=seed)
model.set_params(**best_random)

#Generacion pipeline
best_model = Pipeline(steps=[
("transform", transformer),
("imputer", imputer),
("model", model)])

# Entrenar el mejor modelo con todos los datos
best_model.fit(x_train, y_train)
val_predictions_xgb_opt = best_model.predict(x_val)
mae_xgb_opt = mean_absolute_error(y_val, val_predictions_xgb_opt)
print(f'MAE con RandomForest Optimizado: {mae_xgb_opt}')
print(f'Número de trials: {num_trials}')
#Print parametros
print('')
print('Mejores hiperparámetros:')
for key, value in best_params.items():
    print(f'{key}: {value}')

# Guardar modelo
joblib.dump(best_model, 'best_model_optimized_segundo.pkl')


y_pred_val = best_model.predict(x_val)
y_pred_test = best_model.predict(x_test)

# metricas

conf_matrix_val = confusion_matrix(y_pred_val, y_val)
conf_matrix_val_norm = confusion_matrix(y_pred_val, y_val, normalize='true')
class_report_val = classification_report(y_pred_val, y_val)
precision_val = precision_score(y_pred_val, y_val)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_val = float(accuracy_line.split()[1])

conf_matrix_test = confusion_matrix(y_pred_test, y_test)
conf_matrix_test_norm = confusion_matrix(y_pred_test, y_test, normalize='true')
class_report_test = classification_report(y_pred_test, y_test)
precision_test = precision_score(y_pred_test, y_test)

#Obtenemos acurracy como variable
lines = class_report_val.split('\n')
accuracy_line = lines[5]
accuracy_test = float(accuracy_line.split()[1])

fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_val, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_val_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Validacion')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()


print("\nInforme de clasificación Validacion:")
print(class_report_val)

print("\nPrecision-Score:", precision_val)

print("\n Test:")
fig, ax = plt.subplots(1,2,figsize=(15,5))
sns.heatmap(conf_matrix_test, annot=True, fmt='.3g', ax=ax[0])
sns.heatmap(conf_matrix_test_norm, annot=True, fmt='.3g', ax=ax[1])
ax[0].set_title('Matriz de confusión Test')
ax[1].set_title('Matriz de confusión normalizada')
plt.show()

print("\nInforme de clasificación test:")
print(class_report_test)

print("\nPrecision-Score:", precision_test)

"""## 6. Interpretabilidad [1.0 puntos]

_En esta sección, se espera que los estudiantes demuestren su capacidad para explicar cómo sus modelos toman decisiones utilizando los datos. Dentro del análisis de interpretabilidad propuesto para el modelo, deberán ser capaces de:_

- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.
- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?
- Analizar 10 observaciones aleatorias utilizando un método específico para verificar la coherencia de las interacciones entre las características.
- Explorar cómo se relacionan las variables utilizando algún descriptivo de interpretabilidad.
- ¿Existen variables irrelevantes en el problema modelado?, ¿Cuales son?.

Es fundamental que los estudiantes sean capaces de determinar si su modelo toma decisiones coherentes y evaluar el impacto que podría tener la aplicación de un modelo con esas variables en una población. ¿Es posible que el modelo sea perjudicial o que las estimaciones se basen en decisiones sesgadas?

En resumen, esta sección busca que los estudiantes apliquen un enfoque crítico para evaluar la interpretabilidad de su modelo, identificar posibles sesgos y analizar las implicaciones de sus decisiones en la población objetivo.

- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.

Utilizaremos el metodo de permutation importance, este metodo se acomoda a nuestra desarrollo debido a que utilizamos pipelines y no directamente clasificadores puros, por ello, utilizar shap con treexplainer necesita reacomodar la data. Ademas, en experimentos previos vimos que este metodo entrega buenos resultados independiente del clasificador, en donde utilizaremos el clasificador de RandomForest debido a que nos hace mas sentido sus resultados y sus metricas son buenas independiente de su valor de precision. Cabe destacar que esta decision la tomamos en parte debido a que el primer clasificador redujo mucho sus clasificaciones para clase 1, dicho resultado sucede en ciertas ocaciones en donde se maximiza la precision, debido a que el clasificador toma la decision de no clasificar la clase complicada dado que existe mucho riesgo asociado a cometer un error optando por reducir el accuracy de la clase 0. Basicamente, es una especie de overfitting en donde el clasificador solo clasifica la clase 1 si esta muy seguro de la clasifcacion correcta, dicho riesgo esta asociado a la metrica de confianza.
"""

x_test = x_test.reset_index(drop=True)

#Desarrollo permutation importance
from sklearn.inspection import permutation_importance

results = permutation_importance(best_model, x_test, y_test, n_repeats=30, random_state= seed, n_jobs=-1)
perm_sorted_idx = results.importances_mean.argsort()

# Obtener los nombres de las características
feature_names = x_test.columns

# Visualizar la importancia de las características por incremento del error
plt.boxplot(
    results.importances[perm_sorted_idx].T,
    vert=False,
    labels=np.array(feature_names)[perm_sorted_idx],
)
plt.title("Importancia de las Características por Incremento del Error")
plt.xlabel('Error')
plt.show()

"""- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?

No lo es, la variable outsanding_debt corresponde a la variable mas importante teniendo una gran diferencia respecto al resto de las variables, como por ejemplo amount_invested_monthly. Esto tiene mucho sentido debido a que la variable "outstanding debt" corresponde a si el cliente debe o no dinero, por lo tanto es un indicador demasiado directo de si el cliente es riesgoso o no, basicamente, si posee una deuda es mucho mas probable que dicho cliente no se de fiar. Dicha resolucion es algo amplamiento hablado y en efecto es uno de los principales indicadores que revisan los bancos al momento de prestar dinero, en donde si posees deudas previas es bastante dificil que te permitan acceder a un credito. Este analisis tambien aplica para otras variables, como por ejemplo, credit_history_age el cual esta asociado al historial de credito de los clientes, otro indicador que es ampliamente conocido por los clientes de los bancos, en donde se suele decir que si posees un buen historial de credito eres mas propenso a recibir ofertas de prestamos. Por lo tanto, la distribucion entre las variables es coherente y por ello miso es desigual, dado que existen variables mas relevantes que las otras.

"""

!pip install shap

#Para hacer el analisis por muestra debemos utilizar shap
import shap
import warnings
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")

modelo_solo = best_model.named_steps['model']
explainer = shap.Explainer(modelo_solo)


transformer = ColumnTransformer([
    ('Categoricas', OneHotEncoder(sparse_output=False, handle_unknown="ignore",drop='if_binary'), categorical),
    ('Numericas', RobustScaler(), numerical),
    ('imputer',imputer, numerical)
],remainder='passthrough')

transformer.set_output(transform="pandas")

#Procesamos el conjunto de prueba
x_test_final = transformer.fit(x_test)
x_test_final = transformer.transform(x_test)

# Calcular SHAP values para el conjunto de prueba
shap_values = explainer.shap_values(x_test_final)

x_test

len(valores_indices[:][0])

#generamos 10 muestras aleatorias
random_indices = np.random.choice(len(x_test_final), 10, replace=False)

# Calcular los valores SHAP para estas observaciones
valores_indices = explainer.shap_values(x_test_final.iloc[random_indices])

"""## 7. Concluir [1.0 puntos]

_Aquí deben escribir una breve conclusión del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:_

- ¿Pudieron resolver exitosamente el problema?
- ¿Son aceptables los resultados obtenidos?
- ¿En que medida el EDA ayudó a comprender los datos en miras de generar un modelo predictivo?

Respecto a la clasificación:

- ¿Como fue el rendimiento del baseline para la clasificación?
- ¿Pudieron optimizar el baseline para la clasificación?
- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?

Finalmente:

- ¿Estuvieron conformes con sus resultados?
- ¿Creen que hayan mejores formas de modelar el problema?
- ¿En general, qué aprendieron del proyecto? ¿Qué no aprendieron y les gustaría haber aprendido?

**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.

### Otras Instrucciones

Recordar el uso de buenas prácticas de MLOPS como replicabilidad (fijar semillas aleatorias) o el uso del registro de experimentos (con MLFlow). Si bien son opcionales, es altamente recomendado su uso.

Los resultados obtenidos han demostrado ser satisfactorios y aceptables en gran medida. El análisis exploratorio de datos (EDA) desempeñó un papel crucial al proporcionar una comprensión profunda de los datos, permitiendo identificar las columnas relevantes, comprender la distribución y aplicar filtrados contextuales necesarios para mejorar la calidad del modelo predictivo.

Inicialmente, el baseline para la clasificación mostró un rendimiento decente, sin embargo, mediante la optimización, logramos mejorar significativamente los resultados. De una precisión inicial de 0.823 para la clase 0 y 0.67 para la clase 1, logramos elevar estos valores a 0.823 para la clase 0 y 0.675 para la clase 1, evidenciando una mejora luego de la optimización del modelo.

En general, estamos satisfechos con los resultados obtenidos. Si bien el baseline mostró un rendimiento razonable, las optimizaciones permitieron un avance considerable en la precisión de las predicciones, mejorando la capacidad del modelo para distinguir entre clientes de bajo y alto riesgo crediticio.
Este proyecto nos ha enseñado la importancia del análisis exploratorio de datos como base fundamental para el desarrollo de modelos predictivos. Aprendimos cómo las decisiones tomadas durante el EDA pueden impactar directamente en la calidad del modelo final, junto con diferentes iteraciones constantes en el proceso de modelado, como la importancia de explorar y experimentar con diferentes enfoques para lograr mejoras significativas en los resultados. A pesar de todo, se debe saber que siempre hay espacio para perfeccionar el enfoque y profundizar aún más en algunas técnicas de modelado específicas para este contexto.
"""



"""## 8. Bonus: Implementación de Kedro y FastAPI [1.5 puntos]

**OPCIONAL**

En esta sección se les solicita utilizar las últimas tecnologías vistas en el curso para la productivización del proyecto de ciencia de datos, centrándose en la organización y gestión de los flujos de trabajo a través de componentes y pipelines, más el servicio del modelo a través del desarrollo de una API.

Para esto:

1. Genere un proyecto de `Kedro` en donde separe por responsabilidades los nodos/componentes de su proyecto de ciencia de datos en módulos separados. [1.0 puntos]
2. Genere un servidor basado en `FastAPI` el cuál a través de un método post, reciba un batch de datos y genere predicciones para cada uno de ellos. [0.5 puntos]

Las implementaciones son libres. Es decir, usted decide qué componentes implementar, como usar el catálogo de datos y la parametrización del flujo. Sin embargo, evaluaremos buen uso de los framework, modularización y separación de responsabilidades.
"""

